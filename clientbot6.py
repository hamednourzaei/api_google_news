import asyncio
from pygooglenews import GoogleNews
from datetime import datetime
import json
import uuid
import os
import time
import random
from langdetect import detect, LangDetectException

# Ù„ÛŒØ³Øª User-Agentâ€ŒÙ‡Ø§ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0'
]

# ØªÙ†Ø¸ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡
print("ğŸ“¢ Ø´Ø±ÙˆØ¹ ØªÙ†Ø¸ÛŒÙ… GoogleNews...")
seen_links = set()

def search_news(today):
    items = []
    print("ğŸ“¢ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø®Ø¨Ø§Ø±...")
    
    try:
        # Ø§Ù†ØªØ®Ø§Ø¨ User-Agent ØªØµØ§Ø¯ÙÛŒ
        headers = {'User-Agent': random.choice(USER_AGENTS)} if USER_AGENTS else {}
        gn = GoogleNews(lang='fa', country='IR')
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØ£Ø®ÛŒØ± ØªØµØ§Ø¯ÙÛŒ
        time.sleep(random.uniform(2, 5))  # Ø§ÙØ²Ø§ÛŒØ´ ØªØ£Ø®ÛŒØ± Ø¨Ù‡ 2-5 Ø«Ø§Ù†ÛŒÙ‡
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø®Ø¨Ø§Ø±
        search = gn.search(query='all', when='1d')  # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù…Ø§Ù‡ Ú¯Ø°Ø´ØªÙ‡
        print(f"ğŸ“¢ Ø¯Ø±ÛŒØ§ÙØª {len(search['entries'])} Ø®Ø¨Ø± Ø§Ø² Google News")
        
        if not search['entries']:
            print("âš ï¸ Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø§Ø² API Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¨Ø±Ø±Ø³ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª API ÛŒØ§ Ø§ØªØµØ§Ù„ Ø§ÛŒÙ†ØªØ±Ù†Øª.")
            return items
        
        for entry in search['entries']:
            try:
                pub_datetime = datetime(*entry.published_parsed[:6])
                print(f"ğŸ“¢ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø±: {entry.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')} (ØªØ§Ø±ÛŒØ®: {pub_datetime})")
            except (TypeError, ValueError):
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ§Ø±ÛŒØ® Ø¨Ø±Ø§ÛŒ Ø®Ø¨Ø±: {entry.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')}")
                pub_datetime = datetime.now()
            
            # Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯Ù† Ù„ÛŒÙ†Ú©
            if entry['link'] in seen_links:
                print(f"â„¹ï¸ Ø®Ø¨Ø± ØªÚ©Ø±Ø§Ø±ÛŒ Ø±Ø¯ Ø´Ø¯: {entry.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')}")
                continue
            
            # ØªØ´Ø®ÛŒØµ Ø²Ø¨Ø§Ù† Ø¹Ù†ÙˆØ§Ù† Ø®Ø¨Ø±
            try:
                language = detect(entry['title'])
            except LangDetectException:
                language = 'unknown'
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø²Ø¨Ø§Ù† Ø¨Ø±Ø§ÛŒ Ø®Ø¨Ø±: {entry.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')}")
            
            news_item = {
                "id": str(uuid.uuid4()),
                "title": entry['title'],
                "link": entry['link'],
                "published": pub_datetime.strftime('%Y-%m-%d %H:%M:%S'),
                "source": entry['source']['title'],
                "summary": entry.get('summary', ''),
                "languages": language
            }
            try:
                json.dumps(news_item, ensure_ascii=False)
                items.append(news_item)
                seen_links.add(entry['link'])  # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú© Ø¨Ù‡ seen_links
                print(f"âœ… Ø®Ø¨Ø± Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯: {entry['title']} (Ø²Ø¨Ø§Ù†: {language})")
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø³Ø±ÛŒØ§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø®Ø¨Ø±: {entry['title']}. Ø®Ø·Ø§: {str(e)}")
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø®Ø¨Ø§Ø±: {str(e)}")
        if "429" in str(e) or "Too Many Requests" in str(e):
            print("ğŸ“¢ ØªÙ„Ø§Ø´ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² 10 Ø«Ø§Ù†ÛŒÙ‡...")
            time.sleep(10)
            return search_news(today)  # ØªÙ„Ø§Ø´ Ø¯ÙˆØ¨Ø§Ø±Ù‡
        elif "503" in str(e) or "Service Unavailable" in str(e):
            print("âš ï¸ Ø³Ø±ÙˆØ± Google News Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª. ØªÙ„Ø§Ø´ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² 10 Ø«Ø§Ù†ÛŒÙ‡...")
            time.sleep(10)
            return search_news(today)
    return items

async def news_producer(today):
    file_name = f"news_{today.strftime('%Y-%m-%d')}.json"
    new_items = []
    print(f"ğŸ“¢ Ø´Ø±ÙˆØ¹ ØªØ§Ø¨Ø¹ news_producer Ø¨Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ® {today}...")

    new_items.extend(search_news(today))

    print(f"ğŸ“¢ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¬Ø¯ÛŒØ¯: {len(new_items)}")
    if new_items:
        try:
            print(f"ğŸ“¢ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù†/Ù†ÙˆØ´ØªÙ† ÙØ§ÛŒÙ„: {file_name}")
            if os.path.exists(file_name) and os.path.getsize(file_name) > 0:
                with open(file_name, "r", encoding="utf-8") as f:
                    try:
                        data = json.load(f)
                        print(f"ğŸ“¢ ÙØ§ÛŒÙ„ {file_name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ù…ÙˆØ¬ÙˆØ¯: {len(data)}")
                    except json.JSONDecodeError:
                        print(f"âš ï¸ ÙØ§ÛŒÙ„ {file_name} Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª. Ø´Ø±ÙˆØ¹ Ø¨Ø§ ÙØ§ÛŒÙ„ Ø®Ø§Ù„ÛŒ.")
                        data = []
            else:
                print(f"ğŸ“¢ ÙØ§ÛŒÙ„ {file_name} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª. Ø´Ø±ÙˆØ¹ Ø¨Ø§ ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯.")
                data = []

            data.extend(new_items)
            with open(file_name, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… {len(new_items)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø¯Ø± {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Ø¨Ù‡ {file_name} Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù†ÙˆØ´ØªÙ†/Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ {file_name}: {str(e)}")
    else:
        print(f"â„¹ï¸ Ù‡ÛŒÚ† Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ÛŒ Ø¯Ø± {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ÛŒØ§ÙØª Ù†Ø´Ø¯")

async def main():
    today = datetime.now().date()
    print(f"ğŸ“¢ Ø´Ø±ÙˆØ¹ ØªØ§Ø¨Ø¹ main Ø¨Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ® {today}...")

    file_name = f"news_{today.strftime('%Y-%m-%d')}.json"
    global seen_links
    try:
        print(f"ğŸ“¢ Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„ Ù…ÙˆØ¬ÙˆØ¯: {file_name}")
        if os.path.exists(file_name) and os.path.getsize(file_name) > 0:
            with open(file_name, "r", encoding="utf-8") as f:
                try:
                    data = json.load(f)
                    seen_links = set(item['link'] for item in data)
                    print(f"ğŸ“¢ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø§Ø² ÙØ§ÛŒÙ„ {file_name} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯. ØªØ¹Ø¯Ø§Ø¯: {len(seen_links)}")
                except json.JSONDecodeError:
                    print(f"âš ï¸ ÙØ§ÛŒÙ„ {file_name} Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª. Ø´Ø±ÙˆØ¹ Ø¨Ø§ seen_links Ø®Ø§Ù„ÛŒ.")
                    seen_links = set()
        else:
            print(f"ğŸ“¢ ÙØ§ÛŒÙ„ {file_name} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª. Ø´Ø±ÙˆØ¹ Ø¨Ø§ seen_links Ø®Ø§Ù„ÛŒ.")
            seen_links = set()
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ {file_name}: {str(e)}")
        seen_links = set()

    await news_producer(today)
    print("ğŸ“¢ Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")

if __name__ == "__main__":
    try:
        print("ğŸ“¢ Ø´Ø±ÙˆØ¹ Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª...")
        asyncio.run(main())
    except KeyboardInterrupt:
        print("ğŸ›‘ Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª: {str(e)}")
